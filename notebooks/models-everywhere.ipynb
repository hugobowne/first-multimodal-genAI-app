{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Audio Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key captured successfully!\n",
      "Replicate API key captured successfully!\n",
      "Groq API key captured successfully!\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "# Prompt for the OpenAI API key\n",
    "openai_api_key = getpass.getpass(\"Please enter your OpenAI API key: \")\n",
    "print(\"OpenAI API key captured successfully!\")\n",
    "\n",
    "# Prompt for the Replicate API key\n",
    "replicate_api_key = getpass.getpass(\"Please enter your Replicate API key: \")\n",
    "print(\"Replicate API key captured successfully!\")\n",
    "\n",
    "# Prompt for the Grok API key\n",
    "groq_api_key = getpass.getpass(\"Please enter your Groq API key: \")\n",
    "print(\"Groq API key captured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suno Bark: text to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio_out': 'https://replicate.delivery/czjl/YuNTXATojnb4AxMEmJ4MtHQmtI2ctdCkuSbQbEVUFTuVOO0E/audio.wav'}\n"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "# Create a Replicate client instance with the API token\n",
    "client = replicate.Client(api_token=replicate_api_key)\n",
    "\n",
    "# Define the input parameters for the model\n",
    "input_params = {\n",
    "    \"prompt\": \"Hello, my name is Hugo. And, uh â€” and I like pizza. [laughs] But I also have other interests such as playing tic tac toe.\",\n",
    "    \"text_temp\": 0.7,\n",
    "    \"output_full\": False,\n",
    "    \"waveform_temp\": 0.7,\n",
    "    \"history_prompt\": \"announcer\"\n",
    "}\n",
    "\n",
    "# Run the model using Replicate API\n",
    "try:\n",
    "    output = client.run(\n",
    "        \"suno-ai/bark:b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\",\n",
    "        input=input_params\n",
    "    )\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM output --> Suno bark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "def get_llm_response(user_input):\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[\n",
    "    {\"role\": \"user\", \"content\": user_input}\n",
    "  ]\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo ho, me hearties, gather 'round\n",
      "A tale of pirates on the sea profound\n",
      "With waves a-crashing, winds a-blowing\n",
      "Adventure on the high seas, we be going\n",
      "\n",
      "Chorus:\n",
      "Sail, me lads, on this ship divine\n",
      "Raise the anchor, set the sails, in rhythm we'll chime\n",
      "With hooks for hands and patches for eyes\n",
      "We pirates sail the seven seas, bold and wise\n",
      "\n",
      "Oh, the captain be a fearsome man\n",
      "With a peg leg and a hook, he leads the band\n",
      "His voice rides the wind, as we set sail\n",
      "A true scallywag, he never does fail\n",
      "\n",
      "(Chorus)\n",
      "\n",
      "We plunder and pillage, seeking treasure untold\n",
      "From the Caribbean to lands unknown, bold\n",
      "A chest full of gold, a map to guide the way\n",
      "With hearts full of mischief, we're off to play\n",
      "\n",
      "(Chorus)\n",
      "\n",
      "Through storms and squalls, we'll never back down\n",
      "For the thrill of the chase, we wear our frown\n",
      "With cannons ablaze, we conquer the foe\n",
      "There's no place for weakness in our pirate show\n",
      "\n",
      "(Chorus)\n",
      "\n",
      "So raise your rum, me hearties, in toast\n",
      "To the adventure that lurks along the coast\n",
      "With hearts as wild as the ocean breeze free\n",
      "We pirates will forever roam the endless sea\n",
      "\n",
      "(Chorus)\n"
     ]
    }
   ],
   "source": [
    "song = get_llm_response(\"a short pirates sea shanty\")\n",
    "print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio_out': 'https://replicate.delivery/czjl/Fd8GCWYtW96hGJUa1SmBnKXoO4Q1DyvYawfx2fuMBA2L64QTA/audio.wav'}\n"
     ]
    }
   ],
   "source": [
    "# Define the input parameters for the model\n",
    "input_params = {\n",
    "    \"prompt\": song,\n",
    "    \"text_temp\": 0.7,\n",
    "    \"output_full\": False,\n",
    "    \"waveform_temp\": 0.7,\n",
    "    \"history_prompt\": \"announcer\",\n",
    "   # \"duration\": 30\n",
    "}\n",
    "\n",
    "# Run the model using Replicate API\n",
    "try:\n",
    "    output = client.run(\n",
    "        \"suno-ai/bark:b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\",\n",
    "        input=input_params\n",
    "    )\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is totally bent and makes no sense lolz ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to music w/ meta musicgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://replicate.delivery/yhqm/J5PQpE2tNuqMOlQ0f2DMaX2QayhtfpaPLRwv1LBDJqAk64QTA/out.mp3\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"prompt\": \"Horns and Drums. Edo25 major g melodies that sound triumphant and cinematic. Leading up to a crescendo that resolves in a 9th harmonic\",\n",
    "    \"model_version\": \"stereo-large\",\n",
    "    \"output_format\": \"mp3\",\n",
    "    \"normalization_strategy\": \"peak\"\n",
    "}\n",
    "\n",
    "output = client.run(\n",
    "    \"meta/musicgen:671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb\",\n",
    "    input=input\n",
    ")\n",
    "print(output)\n",
    "#=> \"https://replicate.delivery/pbxt/OeLYIQiltdzMaCex1shlEFy6..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://replicate.delivery/yhqm/04DiXRG65647OxukBKSChc7FzJ0eIQ081jzL4fUmParq74QTA/out.mp3\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"prompt\": \"Ancient Trip Hop with Throat Singing\",\n",
    "    \"model_version\": \"stereo-large\",\n",
    "    \"output_format\": \"mp3\",\n",
    "    \"normalization_strategy\": \"peak\",\n",
    "    \"duration\": 30 \n",
    "}\n",
    "\n",
    "output = client.run(\n",
    "    \"meta/musicgen:671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb\",\n",
    "    input=input\n",
    ")\n",
    "print(output)\n",
    "#=> \"https://replicate.delivery/pbxt/OeLYIQiltdzMaCex1shlEFy6..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to music with riffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': 'https://replicate.delivery/czjl/7NYgDjSfzHwKaqSH5xI6DtIwcWEbH5Qljco9e8WeOjKo3xhmA/gen_sound.wav', 'spectrogram': 'https://replicate.delivery/czjl/uOVUtpY6JsJvERml80llAwMipfltFj9zh9RbVGkLPUP6dcoJA/spectrogram.jpg'}\n"
     ]
    }
   ],
   "source": [
    "output = client.run(\n",
    "    \"riffusion/riffusion:8cf61ea6c56afd61d8f5b9ffd14d7c216c0a93844ce2d82ac1c9ecc9c7f24e05\",\n",
    "    input={\n",
    "        \"alpha\": 0.5,\n",
    "        \"prompt_a\": \"West African Desert Blues\",\n",
    "        \"prompt_b\": \"Throat Singing\",\n",
    "        \"denoising\": 0.75,\n",
    "        \"seed_image_id\": \"vibes\",\n",
    "        \"num_inference_steps\": 50\n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: One prompt to many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"The Waffle House is really messing shit up with pancakes and bacon tonight HOLY MOLEY and there's anarchist jazz also!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://replicate.delivery/pbxt/lEMHeZDieDnX3E8Bx3KTiTeDgkPHi2xA6WdHs6NRDsmq6xhmA/R8__00001_.webp']\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"prompt\": message\n",
    "}\n",
    "\n",
    "output = client.run(\n",
    "    \"fofr/epicrealismxl-lightning-hades:0ca10b1fd361c1c5568720736411eaa89d9684415eb61fd36875b4d3c20f605a\",\n",
    "    input=input\n",
    ")\n",
    "print(output)\n",
    "#=> [\"https://replicate.delivery/pbxt/ulYZRIyAUDYpOZfl7OjhrKx..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio_out': 'https://replicate.delivery/czjl/dzrRuXmL1EYZNxfuRNxINYurL6vXpjaZRR738G5OxS87e4QTA/audio.wav'}\n"
     ]
    }
   ],
   "source": [
    "# Define the input parameters for the model\n",
    "input_params = {\n",
    "    \"prompt\": message,\n",
    "    \"text_temp\": 0.7,\n",
    "    \"output_full\": False,\n",
    "    \"waveform_temp\": 0.7,\n",
    "    \"history_prompt\": \"announcer\",\n",
    "   # \"duration\": 30\n",
    "}\n",
    "\n",
    "# Run the model using Replicate API\n",
    "try:\n",
    "    output = client.run(\n",
    "        \"suno-ai/bark:b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\",\n",
    "        input=input_params\n",
    "    )\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text to music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://replicate.delivery/yhqm/Ium0EE1yDJoALtYICuGsq3eAafq9yLW79MwMHhZbMgL8exhmA/out.mp3\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"prompt\": message,\n",
    "    \"model_version\": \"stereo-large\",\n",
    "    \"output_format\": \"mp3\",\n",
    "    \"normalization_strategy\": \"peak\",\n",
    "    \"duration\": 30 \n",
    "}\n",
    "\n",
    "output = client.run(\n",
    "    \"meta/musicgen:671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb\",\n",
    "    input=input\n",
    ")\n",
    "print(output)\n",
    "#=> \"https://replicate.delivery/pbxt/OeLYIQiltdzMaCex1shlEFy6..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many models at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://replicate.delivery/pbxt/zD6E1bsjewxjQiKhEv2Dt2vCbQ6vzhYbBHLzQTJVA4VxhcoJA/R8__00001_.webp']\n",
      "{'audio_out': 'https://replicate.delivery/czjl/C8USepRYFBSsSCL8HiYEuePFy6X5GwG5BulVEFmxYnqRE5QTA/audio.wav'}\n",
      "https://replicate.delivery/yhqm/ITiK3WFTZxphM9tquShTocicckDXq5B2eEniBaczdSbsicoJA/out.mp3\n"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "def generate_epic_realism(prompt, api_token):\n",
    "    # Create a Replicate client instance with the API token\n",
    "    client = replicate.Client(api_token=replicate_api_key)\n",
    "\n",
    "    # Define the input parameters for the model\n",
    "    input_data = {\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "\n",
    "    # Run the model using Replicate API\n",
    "    output = client.run(\n",
    "        \"fofr/epicrealismxl-lightning-hades:0ca10b1fd361c1c5568720736411eaa89d9684415eb61fd36875b4d3c20f605a\",\n",
    "        input=input_data\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "# api_token = XXX\n",
    "message = \"crazy wild zombie party at the blaring symphony orchestra\"\n",
    "output = generate_epic_realism(message, replicate_api_key)\n",
    "print(output)\n",
    "\n",
    "\n",
    "def generate_suno_bark(prompt, api_token, text_temp=0.7, output_full=False, waveform_temp=0.7, history_prompt=\"announcer\"):\n",
    "    # Create a Replicate client instance with the API token\n",
    "    client = replicate.Client(api_token=replicate_api_key)\n",
    "\n",
    "    # Define the input parameters for the model\n",
    "    input_params = {\n",
    "        \"prompt\": prompt,\n",
    "        \"text_temp\": text_temp,\n",
    "        \"output_full\": output_full,\n",
    "        \"waveform_temp\": waveform_temp,\n",
    "        \"history_prompt\": \"zh_speaker_7\",\n",
    "    }\n",
    "\n",
    "    # Run the model using Replicate API\n",
    "    try:\n",
    "        output = client.run(\n",
    "            \"suno-ai/bark:b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\",\n",
    "            input=input_params\n",
    "        )\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# api_token = 'your_api_token_here'\n",
    "# message = \"Your prompt here\"\n",
    "output = generate_suno_bark(message, replicate_api_key)\n",
    "print(output)\n",
    "\n",
    "import replicate\n",
    "\n",
    "def generate_music_gen(prompt, api_token, duration=30, model_version=\"stereo-large\", output_format=\"mp3\", normalization_strategy=\"peak\"):\n",
    "    # Create a Replicate client instance with the API token\n",
    "    client = replicate.Client(api_token=replicate_api_key)\n",
    "\n",
    "    # Define the input parameters for the model\n",
    "    input_data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model_version\": model_version,\n",
    "        \"output_format\": output_format,\n",
    "        \"normalization_strategy\": normalization_strategy,\n",
    "        \"duration\": duration \n",
    "    }\n",
    "\n",
    "    # Run the model using Replicate API\n",
    "    output = client.run(\n",
    "        \"meta/musicgen:671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb\",\n",
    "        input=input_data\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "# api_token = 'your_api_token_here'\n",
    "# message = \"Ancient Trip Hop with Throat Singing\"\n",
    "output = generate_music_gen(message, replicate_api_key)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio_out': 'https://replicate.delivery/czjl/q5wKXcw1c3ajHRjJz6rk1YzJeIWU3cU6Pf6HtaVSYOZoF5QTA/audio.wav'}\n"
     ]
    }
   ],
   "source": [
    "def generate_suno_bark(prompt, api_token, text_temp=0.7, output_full=False, waveform_temp=0.7, history_prompt=\"announcer\"):\n",
    "    # Create a Replicate client instance with the API token\n",
    "    client = replicate.Client(api_token=replicate_api_key)\n",
    "\n",
    "    # Define the input parameters for the model\n",
    "    input_params = {\n",
    "        \"prompt\": prompt,\n",
    "        \"text_temp\": text_temp,\n",
    "        \"output_full\": output_full,\n",
    "        \"waveform_temp\": waveform_temp,\n",
    "        \"history_prompt\": \"announcer\",\n",
    "    }\n",
    "\n",
    "    # Run the model using Replicate API\n",
    "    try:\n",
    "        output = client.run(\n",
    "            \"suno-ai/bark:b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\",\n",
    "            input=input_params\n",
    "        )\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# api_token = 'your_api_token_here'\n",
    "# message = \"Your prompt here\"\n",
    "output = generate_suno_bark(message, replicate_api_key)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epic Realism Output:\n",
      "['https://replicate.delivery/pbxt/ogO7Lgy5bHpuGp8TLp9TgLwuYrrt9hyUav8ltxlWhAmaRO0E/R8__00001_.webp']\n",
      "Meta MusicGen Output:\n",
      "https://replicate.delivery/yhqm/nJKhZpl5XK5GC5WbMXSmha8QPz1qreC4f5un2nLicYvxG5QTA/out.mp3\n",
      "Suno Bark Output:\n",
      "{'audio_out': 'https://replicate.delivery/czjl/27axINxSwiIpER6ofNkYMn5ywXGJdWoAohCs7fTe2oQWOyhmA/audio.wav'}\n"
     ]
    }
   ],
   "source": [
    "# Define your API token and prompt message\n",
    "# api_token = 'your_api_token_here'\n",
    "message = \"The Waffle House messing it up for real with the pancakes and bacon and punk abstract jazz, yo!\"\n",
    "\n",
    "# Run the Epic Realism model\n",
    "epicrealism_output = generate_epic_realism(message, replicate_api_key)\n",
    "print(\"Epic Realism Output:\")\n",
    "print(epicrealism_output)\n",
    "\n",
    "# Run the Meta MusicGen model\n",
    "musicgen_output = generate_music_gen(message, replicate_api_key)\n",
    "print(\"Meta MusicGen Output:\")\n",
    "print(musicgen_output)\n",
    "\n",
    "# Run the Suno Bark model\n",
    "bark_output = generate_suno_bark(message, replicate_api_key)\n",
    "print(\"Suno Bark Output:\")\n",
    "print(bark_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now define a little LLM utility function to help us out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(user_input):\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[\n",
    "    {\"role\": \"user\", \"content\": user_input}\n",
    "  ]\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In the heart of the ocean, where the waves crash high,\n",
      "There be a tale worth tellin', 'neath the starry sky.\n",
      "Of fearless pirates sailin', on the open sea,\n",
      "Where mermaids sing their melody, as wild and free.\n",
      "\n",
      "(Chorus)\n",
      "Yo-ho, yo-ho, a pirate's life we lead,\n",
      "With mermaids in our sights, on the watery steed.\n",
      "Their voices sweet as honey, their beauty all around,\n",
      "We'll chase those mermaids true, till we've run aground.\n",
      "\n",
      "(Verse 2)\n",
      "Through storms and tempests, our ship sails on,\n",
      "Seeking those mermaids, with their luring song.\n",
      "Their tails a-glimmering, beneath the moonlit night,\n",
      "We'll plow through the waters, searching with delight.\n",
      "\n",
      "(Chorus)\n",
      "Yo-ho, yo-ho, a pirate's life we lead,\n",
      "With mermaids in our sights, on the watery steed.\n",
      "Their voices sweet as honey, their beauty all around,\n",
      "We'll chase those mermaids true, till we've run aground.\n",
      "\n",
      "(Bridge)\n",
      "But heed this warning, me hearties true,\n",
      "For mermaids be deceivers, with eyes so blue.\n",
      "With their siren's call, they'll lure you to your grave,\n",
      "So keep your wits about ye, and be strong and brave.\n",
      "\n",
      "(Verse 3)\n",
      "When the sun sets low, and the stars shine bright,\n",
      "We'll catch a glimpse of mermaids, in the pale moonlight.\n",
      "Their songs will haunt us, as we sail the seven seas,\n",
      "Forever chasing mermaids, 'neath the ocean breeze.\n",
      "\n",
      "(Chorus)\n",
      "Yo-ho, yo-ho, a pirate's life we lead,\n",
      "With mermaids in our sights, on the watery steed.\n",
      "Their voices sweet as honey, their beauty all around,\n",
      "We'll chase those mermaids true, till we've run aground.\n",
      "\n",
      "(Outro)\n",
      "So raise your glasses, me hearties, and sing this seafaring song,\n",
      "Of pirates and mermaids, and adventures bold and long.\n",
      "For in the vast expanse of the ocean's gentle blue,\n",
      "Lies the magic of the mermaids, forever calling you.\n",
      "Suno Bark Output:\n",
      "{'audio_out': 'https://replicate.delivery/czjl/PJgG33Qosu6JJF1wCUZlPaFKGGLJOcEm0sEm6trPHRPASO0E/audio.wav'}\n"
     ]
    }
   ],
   "source": [
    "message = get_llm_response(\"short pirates sea shanty about mermaids\")\n",
    "print(message)\n",
    "# Run the Suno Bark model\n",
    "bark_output = generate_suno_bark(message, replicate_api_key)\n",
    "print(\"Suno Bark Output:\")\n",
    "print(bark_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: text to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://replicate.delivery/yhqm/wx1JqbQ8Nrp9FRCHki5UAtzQZuUUqkvSOp54rNwVQx1rSO0E/out.mp4\n"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "message = \"The Double RR diner messing it up for real with the pancakes and bacon and punk abstract jazz, yo!\"\n",
    "\n",
    "input = {\n",
    "    \"sampler\": \"klms\",\n",
    "    \"max_frames\": 100,\n",
    "    \"animation_prompts\": message\n",
    "}\n",
    "\n",
    "output = client.run(\n",
    "    \"deforum/deforum_stable_diffusion:e22e77495f2fb83c34d5fae2ad8ab63c0a87b6b573b6208e1535b23b89ea66d6\",\n",
    "    input=input\n",
    ")\n",
    "print(output)\n",
    "#=> \"https://replicate.delivery/mgxm/873a1cc7-0427-4e8d-ab3c-..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suno Bark Output:\n",
      "{'audio_out': 'https://replicate.delivery/czjl/wm5zfyqv3K3nSSmN3EgLHZVF9858HgTjQjf1xGHeFx32XyhmA/audio.wav'}\n"
     ]
    }
   ],
   "source": [
    "message = \"giddy up!!! [laughs] time for the thunderdome! [thunder]\"\n",
    "\n",
    "bark_output = generate_suno_bark(message, replicate_api_key)\n",
    "print(\"Suno Bark Output:\")\n",
    "print(bark_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from groq) (2.7.4)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/codespace/.cache/pypoetry/virtualenvs/experiments-in-ai-fJ0XnM_P-py3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.18.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial for several reasons:\n",
      "\n",
      "**1. Real-Time Applications:**\n",
      "\n",
      "* **Chatbots and Conversational AI:**  Speed is essential for natural-sounding conversations. Lag between user input and response makes interactions feel robotic and unnatural.\n",
      "* **Live Transcription and Captioning:** Fast models can provide real-time transcriptions of speech, allowing for immediate accessibility and engagement in live events, meetings, or broadcasts.\n",
      "* **Search Engines:** Faster response times lead to a better user experience, as users expect quick results when searching for information.\n",
      "\n",
      "**2. Scalability and Efficiency:**\n",
      "\n",
      "* **Processing Large Data Sets:**  Fast models can efficiently process massive amounts of text data, enabling large-scale analysis, summarization, and knowledge discovery.\n",
      "* **Resource Efficiency:**  Faster training and inference processes consume less computational power and energy, making them more sustainable and cost-effective.\n",
      "\n",
      "**3. Enhanced User Experience:**\n",
      "\n",
      "* **Improved Responsiveness:**  Users appreciate quick responses, leading to a more engaging and satisfying interaction with AI systems.\n",
      "* **Reduced Latency:**  Low latency is crucial for applications like real-time translation, where even slight delays can be detrimental to communication.\n",
      "\n",
      "**4. Innovation and Development:**\n",
      "\n",
      "* **Faster Research and Prototyping:**  Speed allows researchers to iterate quickly on new models and algorithms, accelerating the pace of AI development.\n",
      "* **Enabling New Applications:** The speed and efficiency of fast language models open up possibilities for novel applications in areas like personalized education, real-time content creation, and interactive storytelling.\n",
      "\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "Despite the numerous benefits, developing fast language models presents challenges:\n",
      "\n",
      "* **Trade-offs with Accuracy:** Speed optimizations can sometimes lead to a slight decrease in accuracy. Striking the right balance is crucial.\n",
      "* **Model Complexity:**  Highly efficient models often require innovative architectures and techniques to achieve fast performance.\n",
      "\n",
      "\n",
      "**Overall, fast language models are essential for advancing AI and creating more interactive, efficient, and impactful applications across various domains.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=groq_api_key)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a transformer-based language model developed and trained by Mistral AI.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"which model are you\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments-in-ai-MDGx_Oj2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
